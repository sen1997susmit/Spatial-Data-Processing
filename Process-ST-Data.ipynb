{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD\n",
    "from sedona.core.SpatialRDD import PointRDD\n",
    "from sedona.core.SpatialRDD import PolygonRDD\n",
    "from sedona.core.SpatialRDD import LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.enums import IndexType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Sedona App\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-2.4_2.11:1.0.0-incubating,org.datasyslab:geotools-wrapper:geotools-24.0\") .\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_WEIGHT = \"weight_type_0\"\n",
    "POINT_DISTANCE = 'weight_type_1'\n",
    "CENTRAL_DISTANCE = 'weight_type_2'\n",
    "EXPONENTIAL_DISTANCE = 'weight_type_3'\n",
    "MINIMUM_DISTANCE = 'weight_type_4'\n",
    "COMMON_BORDER_RATIO = 'weight_type_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_FILE_TYPE_SHAPE = \"geo_file_type_shape\"\n",
    "GEO_FILE_TYPE_GEO_JSON = \"geo_file_type_geo_json\"\n",
    "GEO_FILE_TYPE_WKB = \"geo_file_type_wkb\"\n",
    "GEO_FILE_TYPE_WKT = \"geo_file_type_wkt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Necessary Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to load all different types of spatial data\n",
    "def load_geo_data(path_to_dataset, geo_file_type):\n",
    "    if geo_file_type == GEO_FILE_TYPE_SHAPE:\n",
    "        return ShapefileReader.readToGeometryRDD(sc, path_to_dataset)\n",
    "    elif geo_file_type == GEO_FILE_TYPE_WKB:\n",
    "        return WkbReader.readToGeometryRDD(sc, path_to_dataset, 0, True, False)\n",
    "    elif geo_file_type == GEO_FILE_TYPE_WKT:\n",
    "        return WktReader.readToGeometryRDD(sc, path_to_dataset, 0, True, False)\n",
    "    elif geo_file_type == GEO_FILE_TYPE_GEO_JSON:\n",
    "        return GeoJsonReader.readToGeometryRDD(sc, path_to_dataset)\n",
    "    else:\n",
    "        print(\"Given geo file type is not supported\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get adjacency list between some polygons.\n",
    "# Polygons are in spatial rdd format.\n",
    "def get_adjacency_from_polygons_rdd(geoRdd, weight = BINARY_WEIGHT):\n",
    "    geoRdd.analyze()\n",
    "    \n",
    "    if weight == BINARY_WEIGHT:\n",
    "        # perform spatial join to check which polygons touch which other polygons\n",
    "        consider_boundary_intersection = True\n",
    "        using_index = False\n",
    "        geoRdd.spatialPartitioning(GridType.KDBTREE)\n",
    "        result = JoinQuery.SpatialJoinQuery(geoRdd, geoRdd, using_index, consider_boundary_intersection).collect()\n",
    "        dim = len(result)\n",
    "        adj_mat = [[0]*dim for i in range(dim)]\n",
    "        \n",
    "        for row in result:\n",
    "            i = int(row[0].getUserData())\n",
    "            for col in row[1]:\n",
    "                j = int(col.getUserData())\n",
    "                if i != j:\n",
    "                    adj_mat[i][j] = 1\n",
    "        return adj_mat\n",
    "    \n",
    "    # spatial join is not required for other types of adjacency as we need to calculate distances\n",
    "    elif weight == CENTRAL_DISTANCE or weight == EXPONENTIAL_DISTANCE:\n",
    "        geo_list = geoRdd.rawSpatialRDD.map(lambda x: x.geom.centroid).collect()\n",
    "        dim = len(geo_list)\n",
    "        adj_mat = [[0]*dim for i in range(dim)]\n",
    "        \n",
    "        if weight == CENTRAL_DISTANCE:\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    if i != j:\n",
    "                        adj_mat[i][j] = geo_list[i].distance(geo_list[j])\n",
    "                        \n",
    "        else:\n",
    "            bandwidth = 0\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    if i != j:\n",
    "                        adj_mat[i][j] = geo_list[i].distance(geo_list[j])\n",
    "                        if adj_mat[i][j] > bandwidth:\n",
    "                            bandwidth = adj_mat[i][j]\n",
    "            if bandwidth > 0:\n",
    "                for i in range(dim):\n",
    "                    for j in range(dim):\n",
    "                        adj_mat[i][j] = math.exp((-1 * adj_mat[i][j]**2)/(bandwidth**2))\n",
    "        return adj_mat\n",
    "    \n",
    "    # common border ratio is the ratio of overlap between two polygons vs length of first polygon\n",
    "    elif weight == MINIMUM_DISTANCE or weight == COMMON_BORDER_RATIO:\n",
    "        geo_list = geoRdd.rawSpatialRDD.map(lambda x: x.geom).collect()\n",
    "        dim = len(geo_list)\n",
    "        adj_mat = [[0]*dim for i in range(dim)]\n",
    "        \n",
    "        if weight == MINIMUM_DISTANCE:\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    if i != j:\n",
    "                        adj_mat[i][j] = geo_list[i].distance(geo_list[j])\n",
    "                        \n",
    "        else:\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    if i != j:\n",
    "                        adj_mat[i][j] = geo_list[i].intersection(geo_list[j]).length/geo_list[i].length\n",
    "        return adj_mat\n",
    "    else:\n",
    "        print(\"Given weight type is not supported for points RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get adjacency list between some points.\n",
    "# points are in spatial rdd format.\n",
    "def get_adjacency_from_points_rdd(geoRdd, weight = BINARY_WEIGHT, distanceThreshold = 100):\n",
    "    geoRdd.analyze()\n",
    "    geo_list = geoRdd.rawSpatialRDD.map(lambda x: x.geom).collect()\n",
    "    dim = len(geo_list)\n",
    "    adj_mat = [[0]*dim for i in range(dim)]\n",
    "    \n",
    "    # two points are adjacent if they are within a distance threshold\n",
    "    if weight == BINARY_WEIGHT:\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if i != j and geo_list[i].distance(geo_list[j]) >= distanceThreshold:\n",
    "                    adj_mat[i][j] = 1\n",
    "        return adj_mat\n",
    "        \n",
    "    elif weight == POINT_DISTANCE:\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if i != j:\n",
    "                    adj_mat[i][j] = geo_list[i].distance(geo_list[j])\n",
    "        return adj_mat\n",
    "                        \n",
    "    elif weight == EXPONENTIAL_DISTANCE:\n",
    "        bandwidth = 0\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if i != j:\n",
    "                    adj_mat[i][j] = geo_list[i].distance(geo_list[j])\n",
    "                    if adj_mat[i][j] > bandwidth:\n",
    "                        bandwidth = adj_mat[i][j]\n",
    "        if bandwidth > 0:\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    adj_mat[i][j] = math.exp((-1 * adj_mat[i][j]**2)/(bandwidth**2))\n",
    "        return adj_mat\n",
    "    \n",
    "    else:\n",
    "        print(\"Given weight type is not supported for points RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate binary adjacency of a grid, no spatial operation is required\n",
    "def get_binary_adjacency_from_grid(num_rows, num_cols):\n",
    "    total_cell = num_rows * num_cols\n",
    "    adj_matrix = np.zeros(shape = (total_cell, total_cell))\n",
    "    for i in range(total_cell):\n",
    "        row = math.floor(i/num_cols)\n",
    "        col = i%num_cols\n",
    "        if (col - 1) >= 0:\n",
    "            adj_matrix[i][row * num_cols + (col - 1)] = 1\n",
    "        if (col + 1) < num_cols:\n",
    "            adj_matrix[i][row * num_cols + (col + 1)] = 1\n",
    "        if (row - 1) >= 0:\n",
    "            adj_matrix[i][(row - 1) * num_cols + col] = 1\n",
    "        if (row + 1) < num_rows:\n",
    "            adj_matrix[i][(row + 1) * num_cols + col] = 1\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a list of all geometries given a spatial rdd\n",
    "def get_geometries(geoRdd):\n",
    "    geoRdd.analyze()\n",
    "    return geoRdd.rawSpatialRDD.map(lambda x: x.geom).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the number of partitions, it takes square root of the partition count and create same number of partitions\n",
    "# in both x and y directions. It returns the polygons of partitions as a list.\n",
    "def partition_by_grid(geoRdd, partitions):\n",
    "    geoRdd.analyze()\n",
    "    boundary = geoRdd.boundaryEnvelope\n",
    "    x_arr, y_arr = boundary.exterior.coords.xy\n",
    "    x = list(x_arr)\n",
    "    y = list(y_arr)\n",
    "    minX, minY, maxX, maxY = min(x), min(y), max(x), max(y)\n",
    "    \n",
    "    partitionsAxis = int(math.sqrt(partitions))\n",
    "    intervalX = (maxX - minX)/partitionsAxis\n",
    "    intervalY = (maxY - minY)/partitionsAxis\n",
    "    \n",
    "    polygons = []\n",
    "    for i in range(partitionsAxis):\n",
    "        for j in range(partitionsAxis):\n",
    "            polygons.append(Polygon([(minX + intervalX * i, minY + intervalY * i), (minX + intervalX * (i + 1), minY + intervalY * i), (minX + intervalX * (i + 1), minY + intervalY * (i + 1)), (minX + intervalX * i, minY + intervalY * (i + 1))]))\n",
    "    \n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it creates different number of partitions in x and y directions.\n",
    "# It returns the polygons of partitions as a list.\n",
    "def partition_by_grid_xy(geoRdd, partitionsX, partitionsY):\n",
    "    geoRdd.analyze()\n",
    "    boundary = geoRdd.boundaryEnvelope\n",
    "    x_arr, y_arr = boundary.exterior.coords.xy\n",
    "    x = list(x_arr)\n",
    "    y = list(y_arr)\n",
    "    minX, minY, maxX, maxY = min(x), min(y), max(x), max(y)\n",
    "    \n",
    "    intervalX = (maxX - minX)/partitionsX\n",
    "    intervalY = (maxY - minY)/partitionsY\n",
    "    \n",
    "    polygons = []\n",
    "    for i in range(partitionsX):\n",
    "        for j in range(partitionsY):\n",
    "            polygons.append(Polygon([(minX + intervalX * i, minY + intervalY * i), (minX + intervalX * (i + 1), minY + intervalY * i), (minX + intervalX * (i + 1), minY + intervalY * (i + 1)), (minX + intervalX * i, minY + intervalY * (i + 1))]))\n",
    "    \n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs row normalization of a matrix.\n",
    "def row_normalize(adj_matrix):\n",
    "    # matrix is not converted to rdds as it should be allocatable in memory\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "    sum_of_adj_rows = adj_matrix.sum(axis=1)\n",
    "    adj_matrix_norm = adj_matrix / sum_of_adj_rows[:, np.newaxis]\n",
    "    adj_matrix_norm = np.nan_to_num(adj_matrix_norm, copy = False, nan = 0)\n",
    "    return adj_matrix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates moran's index of an attribute in various spatial zones\n",
    "def get_morans_i(adj_matrix, attr_feature):\n",
    "    # matrix and feature are not converted to rdds as they should be allocatable in memory\n",
    "    total_obs = len(attr_feature)\n",
    "    sum_val = sum(attr_feature)\n",
    "    mean_val = sum_val/total_obs\n",
    "        \n",
    "    sum_square = 0\n",
    "    sum_prod = 0\n",
    "    for i in range(total_obs):\n",
    "        for j in range(total_obs):\n",
    "            sum_prod += adj_matrix[i][j] * (attr_feature[i] - mean_val) * (attr_feature[j] - mean_val)\n",
    "        sum_square += (attr_feature[i] - mean_val) * (attr_feature[i] - mean_val)\n",
    "    morran_i = (total_obs * sum_prod) / (sum_square * np.sum(adj_matrix))\n",
    "    return morran_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spatio-Temporal Sequence Array of Number of Taxi Pickups Happened in Various Spatial Zones at Various Time Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Taxi Trips Shape File and Perform Necessary Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = load_geo_data(\"data/taxi_trip/taxi_zones_2\", GEO_FILE_TYPE_SHAPE)\n",
    "zones.CRSTransform(\"epsg:2263\", \"epsg:2263\")\n",
    "zones_df = Adapter.toDf(zones, spark)\n",
    "zones_df.createOrReplaceTempView(\"zones\")\n",
    "zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_polies = get_geometries(zones)\n",
    "zones_polies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "zones_df = zones_df.drop(\"Shape_Leng\")\n",
    "zones_df = zones_df.drop(\"Shape_Area\")\n",
    "zones_df = zones_df.drop(\"zone\")\n",
    "zones_df = zones_df.drop(\"LocationID\")\n",
    "zones_df = zones_df.drop(\"borough\")\n",
    "zones_df = zones_df.drop(\"OBJECTID\")\n",
    "zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zones_df = zones_df.withColumn(\"_id\", monotonically_increasing_id())\n",
    "#zones_df.printSchema()\n",
    "zones_df = zones_df.rdd.zipWithIndex().toDF()\n",
    "zones_df = zones_df.select(col(\"_1.*\"), col(\"_2\").alias('_id'))\n",
    "zones_df.createOrReplaceTempView(\"zones\")\n",
    "zones_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert taxi zones dataframe into spatial RDD\n",
    "zones_rdd = Adapter.toSpatialRdd(zones_df, \"geometry\")\n",
    "zones_rdd.CRSTransform(\"epsg:2263\", \"epsg:2263\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_rdd.analyze()\n",
    "boundary = zones_rdd.boundaryEnvelope\n",
    "x_arr, y_arr = boundary.exterior.coords.xy\n",
    "x = list(x_arr)\n",
    "y = list(y_arr)\n",
    "minX, minY, maxX, maxY = min(x), min(y), max(x), max(y)\n",
    "print(minX, minY, maxX, maxY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load taxi trip pickup dataset of a month and select necessary columns\n",
    "tripDf = spark.read.format(\"csv\").option(\"delimiter\",\",\").option(\"header\",\"true\").load(\"data/taxi_trip/yellow_tripdata_2009-01.csv\")\n",
    "tripDf = tripDf.select(\"Trip_Pickup_DateTime\", \"Start_Lon\", \"Start_Lat\", \"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\")\n",
    "tripDf.createOrReplaceTempView(\"tripDf\")\n",
    "tripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an id starting from 0 for all pickups\n",
    "tripDf = tripDf.rdd.zipWithIndex().toDF()\n",
    "tripDf = tripDf.select(col(\"_1.*\"), col(\"_2\").alias('Serial_ID'))\n",
    "tripDf.createOrReplaceTempView(\"tripDf\")\n",
    "tripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the pickup time into timestamp format\n",
    "tripDf = tripDf.withColumn(\"pickup_time\", unix_timestamp(\"Trip_Pickup_DateTime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "tripDf = tripDf.drop(\"Trip_Pickup_DateTime\")\n",
    "tripDf.createOrReplaceTempView(\"tripDf\")\n",
    "tripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a backup as we will need it for spatial grid array generation\n",
    "tripDf_backup = tripDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = get_adjacency_from_polygons_rdd(zones_rdd)\n",
    "len(adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "adj_mat = get_adjacency_from_polygons_rdd(zones_rdd, weight = CENTRAL_DISTANCE)\n",
    "t_end = time.time()\n",
    "print(\"Time:\", str(t_end - t_start))\n",
    "len(adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize the pickup data columns we need for spatio-temporal array\n",
    "tripDf = tripDf.select([\"Serial_ID\", \"pickup_time\", \"Start_Lon\", \"Start_Lat\"])\n",
    "tripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate intervals, find min and max of timestamps\n",
    "t_start = time.time()\n",
    "tripDf.createOrReplaceTempView(\"tripDf\")\n",
    "min_max_time = spark.sql('select min(pickup_time), max(pickup_time) from tripDf').collect()[0]\n",
    "minTime = min_max_time[0]\n",
    "maxTime = min_max_time[1]\n",
    "t_end = time.time()\n",
    "print(minTime)\n",
    "print(maxTime)\n",
    "print(\"Time:\", t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ingtervals\n",
    "interval = 3600\n",
    "interval_start = np.arange(minTime, maxTime + 1, interval)\n",
    "interval_end = np.arange(minTime + interval, maxTime + interval + 1, interval)\n",
    "if interval_end[-1] > maxTime + 1:\n",
    "    interval_end[-1] = maxTime + 1\n",
    "print(len(interval_start), len(interval_end))\n",
    "print(interval_start[0], interval_end[0], interval_start[-1], interval_end[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_count = len(interval_start)\n",
    "first_interval = interval_start[0]\n",
    "print(intervals_count)\n",
    "print(first_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_count = zones_rdd.rawSpatialRDD.count()\n",
    "zones_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an interval df to join with taxi df\n",
    "intervalDf = spark.createDataFrame(zip(interval_start.tolist(), interval_end.tolist()), schema=['interval_start', 'interval_end'])\n",
    "intervalDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section finds the number of pickups at various zones at various timestamps\n",
    "buildOnSpatialPartitionedRDD = True\n",
    "usingIndex = True\n",
    "considerBoundaryIntersection = True\n",
    "\n",
    "zones_rdd.analyze()\n",
    "zones_rdd.spatialPartitioning(GridType.KDBTREE, 4)\n",
    "\n",
    "temporalTripDf = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "for i in range(15):\n",
    "    start_id = i * 1000000\n",
    "    end_id = (i + 1) * 1000000 - 1\n",
    "    # join ingtervals with pickups and convert pick location into Spatial points\n",
    "    pointDf = spark.sql(\"select ST_Point(double(tripDf.Start_Lat), double(tripDf.Start_Lon)) as point_loc, tripDf.Serial_ID as Serial_ID, tripDf.pickup_time as pickup_time from tripDf where tripDf.Serial_ID >= {0} and tripDf.Serial_ID <= {1}\".format(start_id, end_id))\n",
    "    pointDf = intervalDf.join(pointDf, [pointDf.pickup_time >=  intervalDf.interval_start, pointDf.pickup_time <  intervalDf.interval_end], \"inner\")\n",
    "    \n",
    "    pointRDD = Adapter.toSpatialRdd(pointDf, \"point_loc\")\n",
    "    pointRDD.CRSTransform(\"epsg:4326\", \"epsg:2263\")\n",
    "    \n",
    "    # perform spatial join to find the number of pickups within various spatial zones\n",
    "    pointRDD.analyze()\n",
    "    pointRDD.spatialPartitioning(zones_rdd.getPartitioner())\n",
    "    pointRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n",
    "    result_pair_rdd = JoinQueryRaw.SpatialJoinQueryFlat(pointRDD, zones_rdd, usingIndex, considerBoundaryIntersection)\n",
    "    pickupInfoPartDf = Adapter.toDf(result_pair_rdd, zones_rdd.fieldNames, pointRDD.fieldNames, spark)\n",
    "    pickupInfoPartDf.createOrReplaceTempView(\"pickupInfoPartDf\")\n",
    "    \n",
    "    # group the pickups based on zones and time intervals\n",
    "    pickupInfoPartDf = spark.sql(\"SELECT int(a._id) as _id, a.interval_start as interval_start, count(a.rightgeometry) as point_cnt FROM pickupInfoPartDf a group by a.interval_start, a._id order by a.interval_start, a._id asc\")\n",
    "    \n",
    "    if i == 0:\n",
    "        temporalTripDf = pickupInfoPartDf\n",
    "    else:\n",
    "        temporalTripDf.union(pickupInfoPartDf)\n",
    "    \n",
    "temporalTripDf.createOrReplaceTempView(\"temporalTripDf\")\n",
    "temporalTripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate pickup counts\n",
    "temporalTripDf = spark.sql(\"SELECT a.interval_start, a._id, sum(a.point_cnt) as point_cnt FROM temporalTripDf a group by a.interval_start, a._id order by a.interval_start, a._id asc\")\n",
    "temporalTripDf.createOrReplaceTempView(\"temporalTripDf\")\n",
    "temporalTripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the final spatio-temporal array\n",
    "st_pickups = [[0]*zones_count for i in range(intervals_count)]\n",
    "\n",
    "t1 = time.time()\n",
    "for row in temporalTripDf.collect(): # toLocalIterator for large rdd or df\n",
    "    st_pickups[(int(row['interval_start']) - first_interval)//interval][row['_id']] = row['point_cnt']\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Required time:\", t2-t1, \"seconds\")\n",
    "print(\"st_pickups shape:\", len(st_pickups), \"x\", len(st_pickups[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the spatio-temporal pickup array as a numpy array into file\n",
    "with open(\"data/taxi_trip/st_pickups.npy\", \"wb\") as f:\n",
    "    np.save(f, st_pickups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spatial Grid Array of Number of Total Taxi Pickup Information in Various Cells of Spatial Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the grid and get the grid polygons\n",
    "grid_poly_list1 = partition_by_grid(zones_rdd, 1000)\n",
    "print(len(grid_ploy_list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ploy_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 75\n",
    "num_cols = 70\n",
    "grid_polies = partition_by_grid_xy(zones_rdd, num_rows, num_cols)\n",
    "print(len(grid_polies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_polies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate id for each of the ploygons\n",
    "_ids = [i for i in range(len(grid_polies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema for polygon dataframe\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"_id\", IntegerType(), False),\n",
    "        StructField(\"geometry\", GeometryType(), False)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polygon daaframe\n",
    "gridPolyDf = spark.createDataFrame(\n",
    "    zip(_ids, grid_polies),\n",
    "    schema = schema\n",
    ")\n",
    "gridPolyDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert grid polygon dataframe into spatial rdd\n",
    "gridPolyRdd = Adapter.toSpatialRdd(gridPolyDf, \"geometry\")\n",
    "gridPolyRdd.CRSTransform(\"epsg:2263\", \"epsg:2263\") # although it seems unnecessary, error occurs without this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripDf = tripDf_backup\n",
    "tripDf.createOrReplaceTempView(\"tripDf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildOnSpatialPartitionedRDD = True\n",
    "usingIndex = True\n",
    "considerBoundaryIntersection = True\n",
    "\n",
    "gridPolyRdd.analyze()\n",
    "gridPolyRdd.spatialPartitioning(GridType.KDBTREE, 4)\n",
    "\n",
    "spatialTripDf = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "for i in range(15):\n",
    "    start_id = i * 1000000\n",
    "    end_id = (i + 1) * 1000000 - 1\n",
    "    pointDf = spark.sql(\"select ST_Point(double(tripDf.Start_Lat), double(tripDf.Start_Lon)) as point_loc, int(tripDf.Passenger_Count) as passenger_count, float(tripDf.Trip_Distance) as trip_dist, float(tripDf.Fare_Amt) as fare from tripDf where tripDf.Serial_ID >= {0} and tripDf.Serial_ID <= {1}\".format(start_id, end_id))\n",
    "    pointRDD = Adapter.toSpatialRdd(pointDf, \"point_loc\")\n",
    "    pointRDD.CRSTransform(\"epsg:4326\", \"epsg:2263\")\n",
    "    \n",
    "    pointRDD.analyze()\n",
    "    pointRDD.spatialPartitioning(gridPolyRdd.getPartitioner())\n",
    "    pointRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n",
    "    result_pair_rdd = JoinQueryRaw.SpatialJoinQueryFlat(pointRDD, gridPolyRdd, usingIndex, considerBoundaryIntersection)\n",
    "    \n",
    "    spatialTripPartDf = Adapter.toDf(result_pair_rdd, zones_rdd.fieldNames, pointRDD.fieldNames, spark)\n",
    "    spatialTripPartDf.createOrReplaceTempView(\"spatialTripPartDf\")\n",
    "    spatialTripPartDf = spark.sql(\"SELECT int(a._id) as _id, count(a.rightgeometry) as point_cnt, sum(a.passenger_count) as passenger_cnt, sum(a.trip_dist) as total_trip_dist, sum(a.fare) as total_fare FROM spatialTripPartDf a group by a._id order by a._id asc\")\n",
    "    \n",
    "    if i == 0:\n",
    "        spatialTripDf = spatialTripPartDf\n",
    "    else:\n",
    "        spatialTripDf.union(spatialTripPartDf)\n",
    "    \n",
    "spatialTripDf.createOrReplaceTempView(\"spatialTripDf\")\n",
    "spatialTripDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attrs = 4\n",
    "grid_trip_info = np.zeros(shape = (num_rows, num_cols, num_attrs))\n",
    "\n",
    "t1 = time.time()\n",
    "for row in spatialTripDf.toLocalIterator(): # toLocalIterator for large rdd or df\n",
    "    _id = row[0]\n",
    "    grid_trip_info[_id//num_cols][_id%num_cols][0] += int(row[1])\n",
    "    grid_trip_info[_id//num_cols][_id%num_cols][1] += int(row[2])\n",
    "    grid_trip_info[_id//num_cols][_id%num_cols][2] += row[3]\n",
    "    grid_trip_info[_id//num_cols][_id%num_cols][3] += row[4]\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Required time:\", t2-t1, \"seconds\")\n",
    "print(\"grid_trip_info shape:\", grid_trip_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the spatial grid array as a numpy array into file\n",
    "with open(\"data/taxi_trip/grid_trip_info.npy\", \"wb\") as f:\n",
    "    np.save(f, grid_trip_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-sedona",
   "language": "python",
   "name": "apache-sedona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
